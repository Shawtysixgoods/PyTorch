
# Руководство по PyTorch: От Основ до Нейронных Сетей

## Введение: Что такое PyTorch?

PyTorch — это мощная библиотека для машинного обучения на Python. Представьте её как универсальный конструктор для создания нейронных сетей. Главная особенность PyTorch — простота и гибкость. Он позволяет легко экспериментировать и быстро создавать прототипы.

## Часть 1: Тензоры — Основа всего

### Что такое тензор?
Тензор — это многомерный массив чисел. Не пугайтесь термина — это просто обобщение понятий, которые вы уже знаете:

- **0D тензор (скаляр)**: одно число — `5.0`
- **1D тензор (вектор)**: список чисел — `[1, 2, 3]`
- **2D тензор (матрица)**: таблица чисел — картинка в оттенках серого
- **3D тензор**: цветная картинка (высота × ширина × цвет)

### Создание тензоров

```python
# Импортируем библиотеку PyTorch - это главный инструмент для работы с нейросетями
# (как молоток для плотника, только для искусственного интеллекта)
import torch

# 1. СОЗДАНИЕ ТЕНЗОРОВ (основной тип данных в PyTorch)
# Тензор - это как многомерная таблица чисел. Бывают разных "размерностей":

# Скаляр - это просто одно число (тензор нулевой размерности)
# Например, температура или цена одного товара
scalar = torch.tensor(7.0)  # создаем тензор из числа 7.0

# Вектор - это одномерный тензор (как список чисел)
# Например, координаты точки в пространстве [x, y, z]
vector = torch.tensor([1.0, 2.0, 3.0])  # создаем тензор из списка чисел

# Матрица - это двумерный тензор (как таблица)
# Например, веса нейросети или изображение в оттенках серого
matrix = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)
# dtype=torch.float32 - указываем, что числа будут дробные (32 бита)
# Это важно для нейросетей - они в основном работают с дробными числами

# 2. СПЕЦИАЛЬНЫЕ ТЕНЗОРЫ (часто используются в нейросетях)
# Случайные тензоры - как случайные числа, но в форме таблицы
# Очень полезны для начальной настройки весов нейросети
# (веса - это как "настройки", которые нейросеть учит)
random_tensor = torch.rand(2, 3)  # матрица 2 строки × 3 столбца со случайными числами от 0 до 1

# Матрица, заполненная нулями - как чистый лист перед началом работы
zeros_tensor = torch.zeros(3, 3)  # 3×3 матрица, где все элементы = 0

# Матрица, заполненная единицами - иногда нужно для специальных операций
ones_tensor = torch.ones(2, 4)    # 2×4 матрица, где все элементы = 1

# 3. ИНФОРМАЦИЯ О ТЕНЗОРАХ
# Перед работой с тензорами полезно проверять их свойства:

# .shape - показывает "форму" тензора (сколько элементов в каждом измерении)
print(f"Форма матрицы: {matrix.shape}")  # выведет (2, 3) - 2 строки, 3 столбца

# .dtype - показывает тип данных в тензоре (целые, дробные числа и т.д.)
print(f"Тип данных: {matrix.dtype}")  # выведет torch.float32 - дробные числа
```

### Основные операции

```python
# Создадим два тензора размером 2x2 (матрицы) для примеров
# Оба будут содержать дробные числа (float32), что типично для нейросетевых вычислений
a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)  # Первая матрица
b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)  # Вторая матрица

# 1. ПОЭЛЕМЕНТНЫЕ ОПЕРАЦИИ (арифметика для каждой пары чисел)
# Эти операции работают ТОЛЬКО с тензорами одинаковой формы

# Сложение - складываем соответствующие элементы матриц
# [1,2]   [5,6]   [1+5, 2+6]   [6, 8]
# [3,4] + [7,8] = [3+7, 4+8] = [10,12]
print("Сложение:", a + b)

# Поэлементное умножение (не путать с матричным умножением!)
# Умножаем соответствующие элементы матриц
# [1,2]   [5,6]   [1*5, 2*6]   [5, 12]
# [3,4] * [7,8] = [3*7, 4*8] = [21,32]
print("Поэлементное умножение:", a * b)

# 2. МАТРИЧНОЕ УМНОЖЕНИЕ (основная операция в нейросетях)
# Это не поэлементная операция! Правило: "строка × столбец"
# Результат будет иметь размер: (строки первой матрицы) × (столбцы второй матрицы)

# Формула для матриц 2×2:
# [a b]   [e f]   [a*e + b*g, a*f + b*h]
# [c d] @ [g h] = [c*e + d*g, c*f + d*h]

# Наш пример:
# [1,2]   [5,6]   [1*5 + 2*7, 1*6 + 2*8]   [19, 22]
# [3,4] @ [7,8] = [3*5 + 4*7, 3*6 + 4*8] = [43, 50]
print("Матричное умножение:", torch.matmul(a, b))  # Официальный способ

# Сокращенная запись (используется чаще)
print("Матричное умножение:", a @ b)  # Значок @ - как "умножение для матриц"
```

### Изменение формы тензоров

```python
# Создаем одномерный тензор (вектор) из чисел от 0 до 11
# torch.arange работает как range() в Python, но создает тензор
# dtype=torch.float32 - делаем числа дробными, так нейросети работают лучше
x = torch.arange(12, dtype=torch.float32)  # Получим [0.0, 1.0, 2.0, ..., 11.0]
print("Исходный тензор:", x)
print("Исходная форма (shape):", x.shape)  # Выведет torch.Size([12]) - один ряд из 12 чисел

# 1. ИЗМЕНЕНИЕ ФОРМЫ (RESHAPE)
# Метод reshape переупаковывает те же данные в новую форму
# Главное правило - общее количество элементов должно сохраняться!
# 12 элементов можно разложить как 3×4, 2×6, но нельзя как 3×5

# Делаем из вектора матрицу 3 строки × 4 столбца
x_reshaped = x.reshape(3, 4)
print("\nПосле reshape(3, 4):")
print(x_reshaped)
print("Новая форма:", x_reshaped.shape)  # Выведет torch.Size([3, 4])

# 2. РАБОТА С РАЗМЕРНОСТЯМИ
# unsqueeze добавляет новую размерность длиной 1 в указанной позиции
# Это как добавить новую ось в наборе данных

# Добавляем нулевую размерность (в начало)
x_expanded = x_reshaped.unsqueeze(0)  # Аргумент 0 - позиция новой размерности
print("\nС дополнительной размерностью (unsqueeze(0)):")
print(x_expanded)
print("Форма:", x_expanded.shape)  # Выведет torch.Size([1, 3, 4])

# Это превратило нашу матрицу 3×4 в "пачку" из одной матрицы 3×4
# Теперь это трехмерный тензор (как книга с 1 страницей)

# Пример добавления размерности в конец (позиция -1)
x_expanded_end = x_reshaped.unsqueeze(-1)
print("\nС размерностью в конце (unsqueeze(-1)):")
print(x_expanded_end)
print("Форма:", x_expanded_end.shape)  # Выведет torch.Size([3, 4, 1])
```

## Часть 2: Автоматическое дифференцирование

Это "мозг" PyTorch, который позволяет нейросетям учиться. Когда вы помечаете тензор как `requires_grad=True`, PyTorch начинает отслеживать все операции с ним.

### Простой пример обучения

```python
# Простейший пример обучения: находим число w, чтобы w*2 = 6
# Это как если бы нейросеть состояла всего из одного параметра w

# Создаем тензор w со значением 1.0 (наша начальная догадка)
# requires_grad=True - говорим PyTorch, что нужно вычислять градиенты для этого тензора
# (градиент - это как "подсказка", в какую сторону менять w, чтобы уменьшить ошибку)
w = torch.tensor(1.0, requires_grad=True)

# Целевое значение, которое мы хотим получить (6.0)
target = torch.tensor(6.0)

# Скорость обучения - насколько сильно мы корректируем w каждый шаг
# (как "шаг" при спуске с горы - слишком большой может перепрыгнуть решение)
learning_rate = 0.1

# Начинаем процесс обучения (10 шагов)
for step in range(10):
    # 1. ПРЯМОЙ ПРОХОД: делаем предсказание
    prediction = w * 2  # Наша "модель" просто умножает w на 2
    
    # 2. ВЫЧИСЛЯЕМ ОШИБКУ (функцию потерь)
    # Используем квадрат разницы между предсказанием и целью
    # (квадрат делает большие ошибки более заметными)
    loss = (prediction - target) ** 2
    
    # Выводим информацию о текущем состоянии
    print(f"Шаг {step}: w = {w.item():.3f}, предсказание = {prediction.item():.3f}, потеря = {loss.item():.3f}")
    
    # 3. ОБРАТНЫЙ ПРОХОД: вычисляем градиенты
    loss.backward()  # PyTorch автоматически вычисляет производные (градиенты)
    
    # 4. ОБНОВЛЯЕМ ПАРАМЕТР w (градиентный спуск)
    # torch.no_grad() - временно отключаем вычисление градиентов для обновления
    with torch.no_grad():
        # Обновляем w: двигаемся в направлении, противоположном градиенту
        # learning_rate контролирует размер шага
        w -= learning_rate * w.grad
    
    # 5. ОБНУЛЯЕМ ГРАДИЕНТЫ (ВАЖНО!)
    # Если не обнулить, градиенты будут накапливаться (суммироваться)
    w.grad.zero_()

# Выводим итоговый результат
print(f"\nИтоговое значение w: {w.item():.3f}")  # Должно быть близко к 3.0
```

## Часть 3: Создание нейронных сетей с nn.Module

### Основы nn.Module

`nn.Module` — это базовый класс для всех нейронных сетей в PyTorch. У него два главных метода:

- `__init__()` — здесь мы определяем слои нашей сети
- `forward()` — здесь мы описываем, как данные проходят через сеть

### Простая нейронная сеть

```python
# Импортируем модуль torch.nn - содержит все необходимые строительные блоки для нейросетей
import torch.nn as nn

# Создаем класс нашей нейронной сети, наследуясь от nn.Module
# (это стандартный способ создания сетей в PyTorch)
class SimpleNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        # Инициализируем родительский класс (обязательно!)
        super().__init__()
        
        # 1. ОПРЕДЕЛЯЕМ СЛОИ НАШЕЙ СЕТИ
        
        # Первый линейный слой (полносвязный):
        # Преобразует входной вектор размера input_size в вектор размера hidden_size
        # Можно представить как матрицу размера (input_size × hidden_size)
        self.linear1 = nn.Linear(input_size, hidden_size)
        
        # Функция активации ReLU (Rectified Linear Unit)
        # Делает сеть нелинейной: заменяет все отрицательные значения на 0
        # Формула: ReLU(x) = max(0, x)
        self.activation = nn.ReLU()
        
        # Второй линейный слой:
        # Преобразует вектор hidden_size в выходной вектор output_size
        self.linear2 = nn.Linear(hidden_size, output_size)
    
    # Метод forward определяет, как данные проходят через сеть
    def forward(self, x):
        # Прямой проход через первый слой
        x = self.linear1(x)
        
        # Применяем функцию активации
        x = self.activation(x)
        
        # Прямой проход через второй слой
        x = self.linear2(x)
        
        return x

# 2. СОЗДАЕМ ЭКЗЕМПЛЯР НАШЕЙ МОДЕЛИ

# Параметры сети:
# input_size=10 - размер входных данных (например, 10 признаков)
# hidden_size=20 - количество нейронов в скрытом слое
# output_size=1 - размер выхода (например, 1 число для регрессии)
model = SimpleNetwork(input_size=10, hidden_size=20, output_size=1)

# Посмотрим, как устроена наша модель
print(model)
# Выведет что-то вроде:
# SimpleNetwork(
#   (linear1): Linear(in_features=10, out_features=20, bias=True)
#   (activation): ReLU()
#   (linear2): Linear(in_features=20, out_features=1, bias=True)
# )

# 3. ПРОВЕРЯЕМ РАБОТУ СЕТИ НА ТЕСТОВЫХ ДАННЫХ

# Создаем случайные входные данные:
# 5 примеров (например, 5 объектов), каждый по 10 признаков
test_input = torch.randn(5, 10)  # randn создает данные из нормального распределения

# Пропускаем данные через сеть (автоматически вызывается forward)
output = model(test_input)

# Смотрим размер выхода - должен быть (5, 1):
# 5 примеров × 1 выходное значение для каждого
print(f"Форма выхода: {output.shape}")
```

### Популярные слои

```python
# 1. ОСНОВНЫЕ СТРОИТЕЛЬНЫЕ БЛОКИ НЕЙРОННЫХ СЕТЕЙ

# Полносвязный слой (линейное преобразование)
# 10 входных признаков → 5 выходных значений
# Внутри содержит матрицу весов размером 5×10 и вектор смещений длиной 5
# Формула: y = x @ W.T + b (где @ - матричное умножение)
linear_layer = nn.Linear(10, 5)

# Функция активации ReLU (Rectified Linear Unit)
# Делает сеть нелинейной: заменяет все отрицательные значения на 0
# Формула: ReLU(x) = max(0, x)
# Важно: не имеет обучаемых параметров
activation = nn.ReLU()

# Слой регуляризации Dropout
# Во время обучения случайно "отключает" 20% нейронов (обнуляет их выход)
# Помогает предотвратить переобучение (когда сеть "запоминает" данные вместо обучения)
# Работает только во время обучения, во время предсказания все нейроны активны
dropout = nn.Dropout(0.2)

# Слой Flatten (выравнивание)
# Превращает многомерный тензор (например, изображение 28×28) 
# в одномерный вектор (784 элемента)
# Пример: из формы (64, 28, 28) делает (64, 784)
flatten = nn.Flatten()

# 2. СОБИРАЕМ СЕТЬ ИЗ КОМПОНЕНТОВ

# nn.Sequential - контейнер, который объединяет слои в последовательность
# Данные проходят через слои в порядке их указания
network = nn.Sequential(
    # Первый слой: 784 входа (например, пиксели изображения 28×28) → 128 нейронов
    nn.Linear(784, 128),
    
    # Функция активации (добавляет нелинейность)
    nn.ReLU(),
    
    # Регуляризация (помогает бороться с переобучением)
    nn.Dropout(0.2),
    
    # Второй слой: 128 нейронов → 10 выходов (например, для 10 классов)
    nn.Linear(128, 10)
)

# Пример архитектуры:
# Вход (784) → [Линейный(128)] → ReLU → Dropout → [Линейный(10)] → Выход

# 3. КАК ЭТО РАБОТАЕТ НА ПРАКТИКЕ
# Представим, что мы обрабатываем изображения рукописных цифр (28×28 = 784 пикселя)
# 1. Изображение преобразуется в вектор из 784 чисел
# 2. Первый слой преобразует его в вектор из 128 чисел
# 3. ReLU удаляет все отрицательные значения
# 4. Dropout случайно обнуляет часть значений
# 5. Второй слой преобразует в вектор из 10 чисел (вероятности для каждой цифры 0-9)
```

## Часть 4: Работа с данными — Dataset и DataLoader

### Зачем нужен DataLoader?

Представьте, что у вас есть миллион фотографий. Загрузить их все в память сразу невозможно. DataLoader решает эту проблему, подавая данные маленькими порциями (батчами).

```python
# Импортируем необходимые модули
from torch.utils.data import DataLoader  # Для загрузки данных батчами
from torchvision import datasets, transforms  # Датасеты и преобразования для изображений

# 1. ПОДГОТОВКА ПРЕОБРАЗОВАНИЙ ДЛЯ ИЗОБРАЖЕНИЙ
# transforms.Compose - цепочка преобразований, которые применяются к данным автоматически
transform = transforms.Compose([
    # Преобразуем изображение (PIL.Image или numpy.array) в тензор PyTorch
    # Дополнительно: меняет формат с [H, W, C] на [C, H, W] и приводит значения к диапазону [0, 1]
    transforms.ToTensor(),
    
    # Нормализуем данные: вычитаем среднее (0.1307) и делим на стандартное отклонение (0.3081)
    # Эти значения специфичны для MNIST (вычислены заранее)
    # Формула: (пиксель - среднее) / стандартное_отклонение
    transforms.Normalize((0.1307,), (0.3081,))
])

# 2. ЗАГРУЗКА ДАТАСЕТА MNIST
# MNIST - датасет рукописных цифр (60,000 обучающих изображений 28x28 пикселей)
train_dataset = datasets.MNIST(
    root='./data',          # Путь для сохранения данных
    train=True,             # Загружаем обучающую часть (False для тестовой)
    download=True,          # Скачать, если нет локальной копии
    transform=transform     # Применяем наши преобразования
)

# 3. СОЗДАНИЕ DATA LOADER
# DataLoader загружает данные батчами и предоставляет много полезных функций
train_loader = DataLoader(
    dataset=train_dataset,  # Наш датасет
    batch_size=64,          # Количество образцов в одном батче
    shuffle=True,           # Перемешивать данные перед каждой эпохой
    num_workers=2           # Количество подпроцессов для загрузки данных
)

# 4. ПРОСМОТР ПЕРВОГО БАТЧА
# DataLoader работает как итератор - можно перебирать батчи в цикле
for images, labels in train_loader:
    # Выводим информацию о первом батче
    print(f"Форма батча изображений: {images.shape}")
    # Выведет: torch.Size([64, 1, 28, 28])
    # 64 изображения, 1 канал (черно-белые), высота 28, ширина 28
    
    print(f"Форма батча меток: {labels.shape}")
    # Выведет: torch.Size([64])
    # 64 соответствующих метки (цифры от 0 до 9)
    
    break  # Прерываем после первого батча, чтобы просто посмотреть пример
```

## Часть 5: Полный цикл обучения

### Пошаговый алгоритм

1. **Подготовка**: загружаем данные, создаем модель, выбираем функцию потерь и оптимизатор
2. **Цикл обучения**: для каждого батча делаем прямой проход, вычисляем потери, делаем обратный проход, обновляем веса
3. **Оценка**: проверяем качество модели на тестовых данных

### Полный пример

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# 1. ПОДГОТОВКА ДАННЫХ =======================================================

# Преобразования для входных изображений
transform = transforms.Compose([
    # Конвертируем изображение в тензор и нормализуем значения пикселей в [0, 1]
    transforms.ToTensor(),
    
    # Нормализуем данные с предварительно вычисленными средним и стандартным отклонением
    # Это помогает сети быстрее обучаться
    transforms.Normalize((0.1307,), (0.3081,))
])

# Загрузка обучающего и тестового наборов данных MNIST
train_dataset = datasets.MNIST(
    './data',                   # Путь для сохранения данных
    train=True,                 # Загружаем обучающую выборку
    download=True,              # Скачиваем, если нет локальной копии
    transform=transform         # Применяем наши преобразования
)

test_dataset = datasets.MNIST(
    './data',
    train=False,                # Загружаем тестовую выборку
    transform=transform
)

# Создаем загрузчики данных (DataLoader)
train_loader = DataLoader(
    train_dataset,
    batch_size=64,              # Размер пакета для обучения
    shuffle=True                # Перемешиваем данные перед каждой эпохой
)

test_loader = DataLoader(
    test_dataset,
    batch_size=1000,            # Размер пакета для тестирования
    shuffle=False               # Для тестирования перемешивание не нужно
)

# 2. СОЗДАНИЕ МОДЕЛИ ========================================================

class MNISTClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        # Преобразуем двумерное изображение в одномерный вектор
        self.flatten = nn.Flatten()
        
        # Архитектура нейронной сети
        self.network = nn.Sequential(
            # Первый полносвязный слой: 784 входа (28*28) → 128 нейронов
            nn.Linear(28*28, 128),
            
            # Функция активации ReLU (добавляет нелинейность)
            nn.ReLU(),
            
            # Регуляризация Dropout (отключает 20% нейронов случайным образом)
            nn.Dropout(0.2),
            
            # Выходной слой: 128 нейронов → 10 выходов (по одному на каждую цифру)
            nn.Linear(128, 10)
        )
    
    def forward(self, x):
        # Преобразуем входные данные
        x = self.flatten(x)      # Из [batch_size, 1, 28, 28] в [batch_size, 784]
        return self.network(x)   # Пропускаем через сеть

# Создаем экземпляр модели
model = MNISTClassifier()

# 3. НАСТРОЙКА ПРОЦЕССА ОБУЧЕНИЯ ============================================

# Функция потерь - CrossEntropyLoss (подходит для задач классификации)
# Она объединяет Softmax и Negative Log-Likelihood Loss
criterion = nn.CrossEntropyLoss()

# Оптимизатор - Adam (адаптивный метод градиентного спуска)
# model.parameters() - все обучаемые параметры сети
# lr=0.001 - скорость обучения (learning rate)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 4. ФУНКЦИЯ ОБУЧЕНИЯ =======================================================

def train_epoch(model, train_loader, criterion, optimizer):
    model.train()  # Переводим модель в режим обучения (важно для Dropout)
    total_loss = 0
    
    # Итерируемся по батчам данных
    for batch_idx, (data, target) in enumerate(train_loader):
        # Обнуляем градиенты с предыдущего шага
        optimizer.zero_grad()
        
        # Прямой проход (forward pass) - вычисляем предсказания
        output = model(data)
        
        # Вычисляем значение функции потерь
        loss = criterion(output, target)
        
        # Обратный проход (backward pass) - вычисляем градиенты
        loss.backward()
        
        # Обновляем веса модели
        optimizer.step()
        
        # Суммируем потери для статистики
        total_loss += loss.item()
        
        # Выводим информацию каждые 100 батчей
        if batch_idx % 100 == 0:
            print(f'Батч {batch_idx}, Потеря: {loss.item():.6f}')
    
    # Возвращаем среднее значение потерь за эпоху
    return total_loss / len(train_loader)

# 5. ФУНКЦИЯ ТЕСТИРОВАНИЯ ===================================================

def test_model(model, test_loader, criterion):
    model.eval()  # Переводим модель в режим оценки (важно для Dropout)
    test_loss = 0
    correct = 0
    
    # Отключаем вычисление градиентов для ускорения и экономии памяти
    with torch.no_grad():
        for data, target in test_loader:
            # Прямой проход
            output = model(data)
            
            # Суммируем потери
            test_loss += criterion(output, target).item()
            
            # Получаем предсказания (индекс максимального значения)
            pred = output.argmax(dim=1)
            
            # Считаем количество верных предсказаний
            correct += pred.eq(target).sum().item()
    
    # Вычисляем точность (accuracy)
    accuracy = 100. * correct / len(test_loader.dataset)
    print(f'Точность: {accuracy:.2f}%')
    return accuracy

# 6. ОСНОВНОЙ ЦИКЛ ОБУЧЕНИЯ =================================================

num_epochs = 5  # Количество проходов по всему набору данных

for epoch in range(num_epochs):
    print(f'\nЭпоха {epoch+1}/{num_epochs}')
    
    # Обучаем модель на одной эпохе
    train_loss = train_epoch(model, train_loader, criterion, optimizer)
    print(f'Средняя потеря при обучении: {train_loss:.6f}')
    
    # Тестируем модель на тестовых данных
    accuracy = test_model(model, test_loader, criterion)

print('\nОбучение завершено!')

# 7. СОХРАНЕНИЕ МОДЕЛИ ======================================================

# Сохраняем только параметры модели (веса)
torch.save(model.state_dict(), 'mnist_model.pth')
print('Модель сохранена!')
```

## Важные советы для начинающих

### 1. Выбор устройства (CPU vs GPU)
```python
# Определяем устройство, на котором будут выполняться вычисления
# Проверяем, доступен ли CUDA (GPU NVIDIA)
# Если доступен - используем GPU ('cuda'), иначе CPU ('cpu')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Используется устройство: {device}')

# ПЕРЕНОС МОДЕЛИ НА УСТРОЙСТВО
# Модель нужно отправить на то же устройство, где находятся данные
model = model.to(device)  # Все параметры модели теперь на выбранном устройстве

# ПРИМЕР ПЕРЕНОСА ДАННЫХ
# Обычно делается внутри цикла обучения при загрузке батча
for data, target in train_loader:
    # Отправляем данные и метки на устройство
    data = data.to(device)      # Изображения на GPU/CPU
    target = target.to(device)  # Метки на GPU/CPU
    
    # Дальнейшие вычисления будут выполняться на выбранном устройстве
    output = model(data)
    loss = criterion(output, target)
    # ...

# ВАЖНЫЕ МОМЕНТЫ:
# 1. Все тензоры для операций должны быть на одном устройстве
# 2. Модель и данные должны быть на одном устройстве
# 3. Нельзя смешивать CPU и GPU в одних операциях

# ПРОВЕРКА ИСПОЛЬЗУЕМОГО УСТРОЙСТВА
print(next(model.parameters()).device)  # Показывает устройство первого параметра модели

# ПЕРЕНОС НА GPU ПРИ СОЗДАНИИ ТЕНЗОРА
# Можно сразу создавать тензоры на нужном устройстве
x = torch.tensor([1, 2, 3], device=device)

# ПРЕИМУЩЕСТВА GPU:
# - До 100x ускорение для матричных операций
# - Позволяет работать с большими батчами
# - Ускоряет обучение сложных моделей

# ЕСЛИ GPU НЕТ:
# Код будет работать на CPU без изменений
# Все операции автоматически выполняются на CPU

# ДЛЯ МНОГОГПУСИСТЕМНЫХ СИСТЕМ:
if torch.cuda.device_count() > 1:
    print(f"Используется {torch.cuda.device_count()} GPU!")
    model = nn.DataParallel(model)  # Параллельные вычисления на нескольких GPU
```

### 2. Сохранение и загрузка моделей
```python
# СОХРАНЕНИЕ МОДЕЛИ ========================================================

# Сохраняем только параметры модели (state_dict)
torch.save(model.state_dict(), 'model.pth')
# model.pth - произвольное имя файла (обычно используют .pth или .pt)

# Что такое state_dict?
# Это словарь Python, который содержит:
# - Все обучаемые параметры модели (веса и смещения)
# - Их названия (какому слою они принадлежат)
# - Не включает саму архитектуру модели!

# Альтернативные способы сохранения:
# 1. Сохранить всю модель (архитектура + параметры)
torch.save(model, 'full_model.pth')  # Не рекомендуется!

# 2. Сохранить с дополнительной информацией
checkpoint = {
    'model_state': model.state_dict(),
    'optimizer_state': optimizer.state_dict(),
    'epoch': epoch,
    'loss': loss
}
torch.save(checkpoint, 'checkpoint.pth')

# ЗАГРУЗКА МОДЕЛИ ========================================================

# 1. Сначала создаем экземпляр модели с ТОЙ ЖЕ архитектурой
model = MNISTClassifier()  # Должен быть тот же класс, что и при сохранении

# 2. Загружаем параметры
model.load_state_dict(torch.load('model.pth'))

# 3. Переводим модель в режим оценки (если будем делать предсказания)
model.eval()  # Важно для слоев Dropout, BatchNorm и т.д.

# Что делает .eval()?
# - Отключает Dropout (используются все нейроны)
# - Фиксирует статистику BatchNorm
# - Ускоряет вычисления (не вычисляются градиенты)

# ПРОВЕРКА ЗАГРУЖЕННОЙ МОДЕЛИ ===========================================

# Пример предсказания с загруженной моделью
with torch.no_grad():  # Отключаем вычисление градиентов для экономии памяти
    sample = torch.randn(1, 1, 28, 28)  # Тестовый пример (1 изображение)
    output = model(sample)
    print(f"Предсказание: {output.argmax().item()}")

# ВАЖНЫЕ МОМЕНТЫ:
# 1. Архитектура модели должна быть идентичной при сохранении и загрузке
# 2. Для предсказаний используйте model.eval() и torch.no_grad()
# 3. При обучении - model.train() чтобы активировать Dropout/BatchNorm

# РАБОТА С GPU/CPU =======================================================

# Если сохраняли модель на GPU, а загружаем на CPU:
model.load_state_dict(torch.load('model.pth', map_location=torch.device('cpu')))

# Если хотим загрузить на GPU:
model.load_state_dict(torch.load('model.pth'))
model = model.to('cuda')

# УСТРАНЕНИЕ ПРОБЛЕМ =====================================================

# Частая ошибка: "Missing key(s) in state_dict"
# Причина: Архитектура модели не совпадает с сохраненной
# Решение: Убедитесь, что класс модели точно такой же

# ЛУЧШИЕ ПРАКТИКИ:
# 1. Сохраняйте отдельно параметры (state_dict), а не всю модель
# 2. Сохраняйте и загружайте на одном типе устройства
# 3. Всегда проверяйте точность загруженной модели на тестовых данных
```

### 3. Общие ошибки и как их избежать

**Забыли обнулить градиенты:**
```python
# НЕПРАВИЛЬНО
for data, target in train_loader:
    output = model(data)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()  # Градиенты накапливаются!

# ПРАВИЛЬНО
for data, target in train_loader:
    optimizer.zero_grad()  # Обнуляем градиенты
    output = model(data)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
```

**Неправильный режим модели:**
```python
# При обучении
model.train()

# При тестировании или использовании
model.eval()
```

**Неправильная форма данных:**
```python
# Проверяйте формы тензоров
print(f"Форма входных данных: {data.shape}")
print(f"Форма выходных данных: {output.shape}")
print(f"Форма меток: {target.shape}")
```

## Что изучать дальше?

1. **Сверточные нейронные сети (CNN)** — для работы с изображениями
2. **Рекуррентные сети (RNN, LSTM)** — для работы с последовательностями
3. **Трансформеры** — современная архитектура для NLP и не только
4. **Трансферное обучение** — использование предобученных моделей
5. **Продвинутые техники** — регуляризация, оптимизация, аугментация данных

## Полезные ресурсы

- [Официальная документация PyTorch](https://pytorch.org/docs/)
- [PyTorch Tutorials](https://pytorch.org/tutorials/)
- [Papers With Code](https://paperswithcode.com/) — для изучения современных архитектур

Помните: машинное обучение — это 80% работы с данными и 20% магии моделей. Начните с простых примеров и постепенно усложняйте задачи. Удачи в изучении!
